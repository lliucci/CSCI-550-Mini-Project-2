---
title: "CSCI 550 - Project 2"
author: "Eliot Liucci, Eric Folsom, Nick Clausen, Christal O'Connell"
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2:
    fig_height: 4
    fig_width: 8
    extra_dependencies: ['amsmath', 'float']
    number_sections: true
    toc: false
fontsize: 12pt
spacing: double
indent: true
header-includes:
  \usepackage{float}
  \floatplacement{figure}{H}
bibliography: mini_project_2.bib
---

```{r, setup, include=FALSE}
require(mosaic)   # Load additional packages here 
knitr::opts_chunk$set(
  tidy=FALSE,     # display code as typed
  size="small",   # slightly smaller font for code
  warning = F,    # display warnings
  message = F,    # display messages
  echo = T,       # display code
  eval = F,       # evaluate code
  fig.pos = "H")  # holdings figures in position

# Packages
library(tidyverse)
library(ggmap)
library(sf)
library(caret)
theme_set(theme_bw())

# Data Reading
data = read_csv("data_cleaned.csv")

# Converting Columns into factors with levels
data <- data %>%
  mutate(Property_Class = factor(Property_Class),
         Neighborhood_Code = factor(Neighborhood_Code),
         Town_Code = factor(Town_Code),
         Apartments = factor(Apartments),
         Wall_Material = factor(Wall_Material),
         Roof_Material = factor(Roof_Material),
         Basement = factor(Basement),
         Basement_Finish = factor(Basement_Finish),
         Central_Heating = factor(Central_Heating),
         Other_Heating = factor(Other_Heating),
         Central_Air = factor(Central_Air),
         Attic_Type = factor(Attic_Type),
         Attic_Finish = factor(Attic_Finish),
         Design_Plan = factor(Design_Plan),
         Cathedral_Ceiling = factor(Cathedral_Ceiling),
         Construction_Quality = factor(Construction_Quality),
         Site_Desirability = factor(Site_Desirability),
         Garage_1_Size = factor(Garage_1_Size),
         Garage_1_Material = factor(Garage_1_Material),
         Garage_1_Attachment = factor(Garage_1_Attachment),
         Garage_1_Area = factor(Garage_1_Area),
         Garage_2_Size = factor(Garage_2_Size),
         Garage_2_Material = factor(Garage_2_Material),
         Garage_2_Attachment = factor(Garage_2_Attachment),
         Garage_2_Area = factor(Garage_2_Area),
         Porch = factor(Porch),
         Other_Improvements = factor(Other_Improvements),
         Repair_Condition = factor(Repair_Condition),
         Multi_Code = factor(Multi_Code),
         Multi_Property_Indicator = factor(Multi_Property_Indicator),
         OHare_Noise = factor(OHare_Noise),
         Floodplain = factor(Floodplain),
         Road_Proximity = factor(Road_Proximity),
         Sale_Quarter_of_Year = factor(Sale_Quarter_of_Year),
         Sale_Half_of_Year = factor(Sale_Half_of_Year),
         Most_Recent_Sale = factor(Most_Recent_Sale),
         Pure_Market_Filter = factor(Pure_Market_Filter),
         Garage_Indicator = factor(Garage_Indicator),
         Fireplaces = factor(Fireplaces),
         Number_of_Commercial_Units = factor(Number_of_Commercial_Units),
         Area = factor(Area),
         Sub_Area = factor(Sub_Area),
         Block = factor(Block),
         Parcel = factor(Parcel),
         Multicode = factor(Multicode)
  )


# Setting up for spatial plots
register_google(key = "AIzaSyCnq3okTIRjxfQ0wHWGyu08HGCrFtIQo4M", write = TRUE)
Map <- get_googlemap(center = c(long = -87.8,lat = 41.8), maptype = "satellite", zoom = 9)
cook_map2 <- read_sf('Congressional_District.geojson')
cook_map_overlay = st_transform(cook_map2)
cook_map <- map_data('county', 'illinois') |> filter(subregion == 'cook')

```

\vfill

# Executive Summary

\newpage  

# Data Preprocessing and Exploration

## Data Cleaning

The data received had a few issues that needed to be dealt with. First of all, the `Description` variable contained pieces of key information that were extracted (number of bathrooms, number of bedrooms, total number of rooms, and sell date). Additionally, Area, Sub-Area, Block, Parcel, and Multicode were parsed from the `PIN` variable.

```{r}
# Extracting important information from descriptions
sell_date = as.numeric(0)
rooms = as.numeric(0)
bedrooms = as.numeric(0)
baths = as.numeric(0)

# Takes a minute to run, not too bad though
for(i in 1:nrow(data)){
    sell_date[i] = str_split(str_split(data$Description[i], "sold on ")[[1]][2], ", is a")[[1]][1]
    rooms[i] = str_extract_all(str_split(data$Description[i], "total of ")[[1]][2], "\\d")[[1]][1]
    bedrooms[i] = str_extract_all(str_split(data$Description[i], "total of ")[[1]][2], "\\d")[[1]][2]
    baths[i] = str_split(str_split(data$Description[i], "bedrooms, and ")[[1]][2], " of which are bathrooms")[[1]][1]
    Area[i] = substring(data$PIN[i], first = 1, last = 2)
    Sub_Area[i] = substring(data$PIN[i], first = 3, last = 4)
    Block[i] = substring(data$PIN[i], first = 5, last = 6)
    Parcel[i] = substring(data$PIN[i], 7, 8)
    Multicode[i] = substring(data$PIN[i], 9, 12)
}
```

Once these variables were extracted, the `Description` variable was dropped from the data set. We noticed some houses with strange recording like "42 bathrooms and 7 rooms", so we dropped any listings where the number of bedrooms and number of bathrooms was greater than the total recorded number of rooms.

```{r}
# Adding features of descriptions, removing descriptions
data = data %>%
    mutate(
      Sell_Date = mdy(sell_date),
      Rooms = as.numeric(rooms),
      Bedrooms = as.numeric(bedrooms),
      Baths = as.numeric(baths)
    ) %>%
    select(-Description)

# Removing rows where num bathrooms/bedrooms exceeds number of rooms
data = data %>%
    filter(Bedrooms < Rooms | Baths < Rooms)
```

Next, we dropped any listings with a missing value in at least one of the variables.

```{r}
# Removing rows with at least 1 missing value
data = data %>%
    drop_na()
```

We also wanted to deal with outliers, so a function was written that would identify a listing as an outlier if it was greater than 3 standard errors away from the mean value and used this to remove outliers for variables where the maximum value was significantly higher than the 3rd quartile.

```{r}
# Filtering extreme observations
is_outlier = function(x){
    result = abs(x - mean(x)) > 3*sd(x)
    return(result)
}

# Removing outliers for variables where max() is greater than q3()
data = data %>%
    filter(!is_outlier(`Sale Price`),
           !is_outlier(`Land Square Feet`),
           !is_outlier(Baths),
           !is_outlier(`Lot Size`),
           !is_outlier(`Town and Neighborhood`),
           !is_outlier(`Age Decade`),
           !is_outlier(`Age`),
           !is_outlier(`Estimate (Land)`),
           !is_outlier(`Estimate (Building)`),
           !is_outlier(`Building Square Feet`),
           !is_outlier(`Other Improvements`))
```

Finally, we removed all spaces from variable names and replaced them with underscores.

```{r}
# Removing Spaces
data = data %>%
  rename(Property_Class = `Property Class`,
        Neighborhood_Code = `Neighborhood Code`,
        Land_Square_Feet = `Land Square Feet`,
        ...
        Age_Decade = `Age Decade`,
        Neighborhood_Code_Mapping = `Neigborhood Code (mapping)`,
        Town_and_Neighborhood = `Town and Neighborhood`,
        )
```

This cleaned data set was written as `data_cleaned.csv` so it could easily be reloaded for the remainder of the analysis.

```{r}
# Save cleaned data
write_csv(data, "data_cleaned.csv")
```

## Exploration of Data

Within the data, latitude and longitude coordinates were provided for each listing. A map of the sale price of listings is overlaid on a satellite map of the region (Figure \@ref(fig:SalePriceSatMap)). Here, it can be seen that a lot of the higher price listings are on the water, with sale price generally decreasing the more in-land the listing is.

```{r SalePriceSatMap, fig.cap = "A spatial map of the sale prices over the region.", fig.height = 8, eval = T}
# Spatial Map of Sale Price
ggmap(Map, darken = c(0.1, "white")) +
    geom_polygon(data = cook_map, aes(x = long, y = lat), 
        fill = NA, color = "orange") +
    geom_point(data = data, 
                aes(x = Longitude, 
                    y = Latitude, 
                    color = Sale_Price), 
                size = 0.02, 
                alpha = 0.75) +
    scale_color_gradient(low = "#6fe7f7", high = "#890000") +
    labs(x = "Longitude", y = "Latitude", color = "Sale Price ($)")
```

Multiple boxplots were created for all of the variables pertaining to a garage against the response (Figure \@ref(fig:GarageVarBoxplots)). As the first garage increases in size, so too does the sale price. Having a garage attached to the building (`Garage_#_Attachement` = 1) is also associated with a higher sale price for both garage 1 and garage 2. Higher quality materials (larger values of `Garage_#_Material`) is associated with higher sale prices too.

```{r GarageVarBoxplots, fig.cap = "A series of boxplots for all garage variables against sale price.", eval = T}
# Plotting Against Garage Variables
data %>%
    select(Sale_Price, 
           Garage_1_Area, 
           Garage_1_Size, 
           Garage_1_Attachment, 
           Garage_1_Material, 
           Garage_2_Area, 
           Garage_2_Size, 
           Garage_2_Attachment, 
           Garage_2_Material) %>%
    pivot_longer(cols = 2:9, names_to = "Variable", values_to = "Value") %>%
    ggplot(aes(x = factor(Value), y = Sale_Price, group = Value)) +
    geom_boxplot() +
    facet_wrap(~Variable, scales = "free_x", nrow = 2) +
    labs(x = " ")
```

When comparing sale price to variables related to the physical property, it can be noted that for the `Apartments` variable, there is large varibility in the "0" group, which may be due to non-apartment buildings being more expensive (Figure \@ref(fig:PropertyVarBoxplots)). Sale price is generally the same across attic types, porch groups, and design plans. Sale price appears to increase as the number of fireplaces increases. Additionally, sale price is higher for listings with cathedral ceilings. The `Proprty_Class` variable apperas to have equal variability in sale price for all classes except "209".

```{r PropertyVarBoxplots, fig.cap = "A series of boxplots for variables related to the property itself.", eval = T}
data %>%
    select(`Sale_Price`, 
            `Property_Class`,
            `Apartments`,
            `Basement`,
            `Attic_Type`,
            `Design_Plan`,
            `Cathedral_Ceiling`,
            Fireplaces,
            Porch) %>%
    pivot_longer(cols = 2:9, names_to = "Variable", values_to = "Value") %>%
    ggplot(aes(x = factor(Value), y = Sale_Price, group = Value)) +
    geom_boxplot() +
    facet_wrap(~Variable, scales = "free_x", nrow = 2) +
    labs(x = " ")
```

Additional boxplots were created for the number of rooms in each listing (Figure \@ref(fig:RoomVarBoxplots)). For all room variables, there is generally an increase in sale price as the number of rooms increases. However, the sale price for listings with 1 room is higher, on average, than all other room groups. The same goes for listings with 0 bedrooms.

```{r RoomVarBoxplots, fig.cap = "A series of boxplots for the variables relating to number of rooms present.", eval = T}
# Plotting Against Room Variables
data %>%
    select(`Sale_Price`, 
            `Bedrooms`,
            `Baths`,
            `Rooms`) %>%
    pivot_longer(cols = 2:4, names_to = "Variable", values_to = "Value") %>%
    ggplot(aes(x = factor(Value), y = Sale_Price, group = Value)) +
    geom_boxplot() +
    facet_wrap(~Variable, scales = "free_x") +
    labs(x = " ")
```

The final visualization of interest is a scatterplot of sale price against both building square footage and land square footage (Figure \@ref(fig:BuildingLandSize)). Here, it can be seen that for listings with high building square footage, we see higher sale price. The relationship is the same for land square footage, although the highest sale prices occur at high building square footage and average land square footage.

```{r BuildingLandSize, fig.cap = "A scatterplot of the relationship between sale price and building/land square footage.", eval = T}
data %>%
    ggplot(aes(x = `Building_Square_Feet`, 
                y = `Land_Square_Feet`, 
                color = `Sale_Price`)) +
    geom_point(size = 2) +
    labs(x = "Building Square Footage",
        y = "Land Square Footage",
        color = "Sale Price ($)")
```

## Hypothesis Development

From the exploration performed above, we hypothesize that building square footage, land square footage, the number of rooms, and location are likely going to have the highest impact on sale price.

# Model Development and Performance Evaluation

Before starting the modelling process, variables were factored (if categorical) or scaled to have mean of 0 and variance 1 (if quantitative). This ensured that models had the best chance of converging. Additionally, some variables were removed due to having only 1 value or being of no importance.

```{r eval = T}
# Scaling numeric variable
Model_Data = data %>%
    select(-c(Modeling_Group,
            Age,
            Use,
            Sale_Half_of_Year,
            `...1`,
            PIN,
            Census_Tract,
            Deed_No,
            Town_and_Neighborhood,
            Neighborhood_Code_Mapping)) %>%
    mutate(Land_Square_Feet = scale(Land_Square_Feet),
            Building_Square_Feet = scale(Building_Square_Feet),
            Estimate_Land = scale(Estimate_Land),
            Estimate_Building = scale(Estimate_Building),
            Lot_Size = scale(Lot_Size))

# Adding polynomial terms for later use
poly_predictors = data.frame(poly(Model_Data$Estimate_Building, 4))

poly_predictors %>% head()

Polynomial_Data = poly_predictors %>%
    rename(Poly_1 = X1,
           Poly_2 = X2,
           Poly_3 = X3,
           Poly_4 = X4) %>%
    bind_cols(Model_Data)
```

At this point, the data were split into training and testing sets by randomly sampling 80% of the rows for the training set and leaving the remaining rows for the testing set.

```{r}
# Sampling rows at random
ids_train = sample(1:nrow(Model_Data), 
                size = round(0.8*nrow(Model_Data)), 
                replace = FALSE)

# Splitting data
train = Model_Data[ids_train,]
test = Model_Data[-ids_train,]
```

To find the best Simple Linear Regression model, all single predictor models were searched and compared on AIC. The top-performing model involved the predictor `Estimate_Building` to predict sale price. A summary of the model fit using K-fold cross-validation is shown below. Note that the $R^2$ indicates fairly poor model fit, but this is to be expected as it is only a single predictor.

```{r eval = T}
# Define cross-validation method with 5 folds
train_control <- trainControl(method = "cv", 
                              number = 5)
 
 
# Build model using linear model
slr <- train(Sale_Price~Estimate_Building, data = train, 
               trControl = train_control, 
               method = "lm")

# Summary of model
print(slr)

slr_preds = predict(slr, test)
print(paste("MSE from Test Set: ", Metrics::mse(slr_preds, test$Sale_Price)))
```

To find the best Multiple Linear Regression model, we used all available predictors with no interactions. Note that no model summary .

```{r eval = T}

mlr <- train(Sale_Price~., data = train, 
               trControl = train_control, 
               method = "lm")

print(mlr)

mlr_preds = predict(mlr, test)
print(paste("MSE from Test Set: ", Metrics::mse(mlr_preds, test$Sale_Price)))
```

```{r eval = T}
# Calculating principle components using numeric data
numeric_data = Model_Data %>% select_if(is.numeric) %>% select(-Sale_Price)
PCs = prcomp(numeric_data)

# Determining how many PCs to use
PCs$sdev^2/sum(PCs$sdev^2)

# Obtain first principle component as data
PC_data = data.frame(PCs$x[,1])

# Convert into data frame, adding response
PC_data = bind_cols(PC_data, data.frame(Sale_Price = Model_Data$Sale_Price))

# Split into test and train
PC_train = PC_data[ids_train,]
PC_test = PC_data[-ids_train,]

# train model with k-fold cv
pc_slr <- train(Sale_Price~., data = PC_train, 
               trControl = train_control, 
               method = "lm")

# display model
print(pc_slr)

# predict to test data
pc_slr_preds = predict(pc_slr, PC_test)
print(paste("MSE from Test Set: ", Metrics::mse(pc_slr_preds, PC_test$Sale_Price)))

# Loadings from first PC
PCs$rotation[,1]
```

```{r eval = T}
poly_train = Polynomial_Data[ids_train,]
poly_test = Polynomial_Data[-ids_train,]

# Training 5th degree polynomial
poly_slr = train(Sale_Price ~ Poly_1 + Poly_2 + Poly_3 + Poly_4, 
                data = poly_train,
                trControl = train_control,
                method = 'lm')


# Summary of model
print(poly_slr)

# Predicting and Calculating MSE
poly_slr_preds = predict(poly_slr, poly_test, type = 'raw')
print(paste("MSE from Test Set: ", Metrics::mse(poly_slr_preds, poly_test$Sale_Price)))
```
